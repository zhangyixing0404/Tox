{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\yzhang\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py35\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Concatenate, Conv1D, Activation, TimeDistributed, Flatten, RepeatVector, Permute,multiply\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, Dropout, GRU, GlobalAveragePooling1D, MaxPooling1D, SpatialDropout1D, BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embeddings vectors\n",
      "mean text len: 67.27352714465661\n",
      "max text len: 1411\n"
     ]
    }
   ],
   "source": [
    "print('loading embeddings vectors')\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(' ')) for o in open('glove.840B.300d.txt',encoding=\"utf8\"))\n",
    "\n",
    "min_count = 10 #the minimum required word frequency in the text\n",
    "max_features = 120000 #it's from previous run with min_count=10\n",
    "maxlen = 180 #padding length\n",
    "num_folds = 5 #number of folds\n",
    "embed_size = 300 #embeddings dimension\n",
    "\n",
    "#sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"\").values\n",
    "\n",
    "print('mean text len:',train[\"comment_text\"].str.count('\\S+').mean())\n",
    "print('max text len:',train[\"comment_text\"].str.count('\\S+').max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words 96144\n"
     ]
    }
   ],
   "source": [
    "tokenizer1 = Tokenizer()\n",
    "tokenizer1.fit_on_texts(list(list_sentences_train)) #  + list(list_sentences_test)\n",
    "num_words = sum([1 for _, v in tokenizer1.word_counts.items() if v >= 2])\n",
    "print('num_words',num_words)\n",
    "max_features = num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding sequences\n",
      "numerical variables\n",
      "create embedding matrix\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train)) # + list(list_sentences_test)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "print('padding sequences')\n",
    "X_train = {}\n",
    "X_test = {}\n",
    "X_train['text'] = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "X_test['text'] = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "print('numerical variables')\n",
    "train['num_words'] = train.comment_text.str.count('\\S+')\n",
    "test['num_words'] = test.comment_text.str.count('\\S+')\n",
    "train['num_comas'] = train.comment_text.str.count('\\.')\n",
    "test['num_comas'] = test.comment_text.str.count('\\.')\n",
    "train['num_bangs'] = train.comment_text.str.count('\\!')\n",
    "test['num_bangs'] = test.comment_text.str.count('\\!')\n",
    "train['num_quotas'] = train.comment_text.str.count('\\\"')\n",
    "test['num_quotas'] = test.comment_text.str.count('\\\"')\n",
    "train['avg_word'] = train.comment_text.str.len() / (1 + train.num_words)\n",
    "test['avg_word'] = test.comment_text.str.len() / (1 + test.num_words)\n",
    "#print('sentiment')\n",
    "#train['sentiment'] = train.comment_text.apply(lambda s : sia.polarity_scores(s)['compound'])\n",
    "#test['sentiment'] = test.comment_text.apply(lambda s : sia.polarity_scores(s)['compound'])\n",
    "scaler = MinMaxScaler()\n",
    "X_train['num_vars'] = scaler.fit_transform(train[['num_words','num_comas','num_bangs','num_quotas','avg_word']])\n",
    "X_test['num_vars'] = scaler.transform(test[['num_words','num_comas','num_bangs','num_quotas','avg_word']])\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "print('create embedding matrix')\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding sequences\n"
     ]
    }
   ],
   "source": [
    "max_features_char = 100\n",
    "maxlen_char = 800\n",
    "tokenizer2 = Tokenizer(num_words=max_features_char, char_level=True)\n",
    "tokenizer2.fit_on_texts(list(list_sentences_train) + list(list_sentences_test))\n",
    "list_tokenized_train2 = tokenizer2.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test2 = tokenizer2.texts_to_sequences(list_sentences_test)\n",
    "print('padding sequences')\n",
    "X_train['char'] = sequence.pad_sequences(list_tokenized_train2, maxlen=maxlen_char, padding='pre', truncating='pre')\n",
    "X_test['char'] = sequence.pad_sequences(list_tokenized_test2, maxlen=maxlen_char, padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_size_char = 16\n",
    "batch_size = 32\n",
    "epochs = 4\n",
    "filter_sizes = [1,2,3,5]\n",
    "num_filters = 32\n",
    "filter_sizes_char = [3,5,7,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start modeling\n",
      "Epoch 1/4\n",
      " 14880/127656 [==>...........................] - ETA: 41:02 - loss: 0.0851 - acc: 0.9731"
     ]
    }
   ],
   "source": [
    "def get_model_cnn(X_train):\n",
    "    global embed_size, embed_size_char\n",
    "    inp = Input(shape=(maxlen, ), name=\"text\")\n",
    "    inp2 = Input(shape=[X_train[\"char\"].shape[1]], name=\"char\")\n",
    "    \n",
    "    char_emb = Embedding(max_features_char, embed_size_char)(inp2)\n",
    "    char_emb = SpatialDropout1D(0.5)(char_emb)\n",
    "    \n",
    "    conv_0_char = Conv1D(num_filters, kernel_size=filter_sizes_char[0], kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(char_emb)\n",
    "    conv_1_char = Conv1D(num_filters, kernel_size=filter_sizes_char[1], kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(char_emb)\n",
    "    conv_2_char = Conv1D(num_filters, kernel_size=filter_sizes_char[2], kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(char_emb)\n",
    "    conv_3_char = Conv1D(num_filters, kernel_size=filter_sizes_char[3], kernel_initializer='normal',\n",
    "                                                                                    activation='elu')(char_emb)\n",
    "    maxpool_0_char = MaxPooling1D(pool_size=maxlen_char - filter_sizes_char[0] + 1)(conv_0_char)\n",
    "    maxpool_1_char = MaxPooling1D(pool_size=maxlen_char - filter_sizes_char[1] + 1)(conv_1_char)\n",
    "    maxpool_2_char = MaxPooling1D(pool_size=maxlen_char - filter_sizes_char[2] + 1)(conv_2_char)\n",
    "    maxpool_3_char = MaxPooling1D(pool_size=maxlen_char - filter_sizes_char[3] + 1)(conv_3_char)\n",
    "        \n",
    "    z_char = Concatenate(axis=1)([maxpool_0_char, maxpool_1_char, maxpool_2_char, maxpool_3_char])   \n",
    "    z_char = Flatten()(z_char)\n",
    "    z_char = Dropout(0.1)(z_char)\n",
    "    \n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable = False)(inp)\n",
    "    x = SpatialDropout1D(0.25)(x)\n",
    "    x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "    #x = Dropout(0.05)(x)\n",
    "    #x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPool1D()(x)\n",
    "    x = Concatenate()([avg_pool, max_pool]) \n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    outp = Concatenate()([x,z_char])\n",
    "    #outp = Dropout(0.3)(outp)    \n",
    "    outp = Dense(6, activation=\"sigmoid\")(outp)\n",
    "    \n",
    "    model = Model(inputs=[inp,inp2], outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model      \n",
    "\n",
    "print('start modeling')\n",
    "scores = []\n",
    "predict = np.zeros((test.shape[0],6))\n",
    "oof_predict = np.zeros((train.shape[0],6))\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=6666)\n",
    "for train_index, test_index in kf.split(X_train['num_vars']):\n",
    "    kfold_X_train = {}\n",
    "    kfold_X_valid = {}\n",
    "    y_train,y_test = y[train_index], y[test_index]\n",
    "    for c in ['text','num_vars','char']:\n",
    "        kfold_X_train[c] = X_train[c][train_index]\n",
    "        kfold_X_valid[c] = X_train[c][test_index]\n",
    "\n",
    "    model = get_model_cnn(X_train)\n",
    "    model.fit(kfold_X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "    predict += model.predict(X_test, batch_size=1000) / num_folds\n",
    "    oof_predict[test_index] = model.predict(kfold_X_valid, batch_size=1000)\n",
    "    cv_score = roc_auc_score(y_test, oof_predict[test_index])\n",
    "    scores.append(cv_score)\n",
    "    print('score: ',cv_score)\n",
    "\n",
    "print('Total CV score is {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
